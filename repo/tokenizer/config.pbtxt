name: "tokenizer"
backend: "python"
max_batch_size: 0
default_model_filename: "tokenizer.py"
input: [
    {
        name: "prompts",
        data_type: TYPE_STRING,
        dims: [-1,1]
    }
]
output: [
    {
        name: "input_ids",
        data_type: TYPE_INT32,
        dims: [-1, 2, 77 ]
    },
    {
        name: "attention_mask",
        data_type: TYPE_INT32,
        dims: [-1, 2, 77 ]
    },
    {
        name: "sample",
        data_type: TYPE_FP32,
        dims: [-1, 4, 64, 64]
    },
    {
        name: "timestep",
        data_type: TYPE_INT32,
        dims: [1]
    },
    {
        name: "i",
        data_type: TYPE_UINT8,
        dims: [1]
    }
]
